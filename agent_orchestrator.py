"""
æ™ºèƒ½ä½“ç¼–æ’å™¨
è´Ÿè´£ FSM çŠ¶æ€ç®¡ç†ã€æ™ºèƒ½ä½“è·¯ç”±ã€ä¸Šä¸‹æ–‡æ‰“åŒ…å’Œ LLM è°ƒç”¨
"""
import os
import time
from typing import Dict, List, Optional, Tuple
from openai import OpenAI
from config import STATE_AGENT_MAPPING, AGENT_DISPLAY_NAMES, OPENAI_CONFIG
from prompt_manager import prompt_manager
from db import db


class AgentOrchestrator:
    """æ™ºèƒ½ä½“ç¼–æ’å™¨"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–ç¼–æ’å™¨
        
        Args:
            api_key: OpenAI API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–
        """
        self.api_key = api_key or OPENAI_CONFIG['api_key']
        if not self.api_key:
            raise ValueError("æœªè®¾ç½® API Keyï¼Œè¯·åœ¨ api_config.json ä¸­é…ç½®æˆ–è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        # æ„å»º client å‚æ•°ï¼ˆåªåŒ…å« OpenAI å®¢æˆ·ç«¯æ”¯æŒçš„å‚æ•°ï¼‰
        client_kwargs = {
            'api_key': self.api_key,
            'timeout': OPENAI_CONFIG.get('timeout', 60),
        }
        
        # æ·»åŠ è‡ªå®šä¹‰ base_urlï¼ˆå¦‚æœæœ‰ï¼‰
        if OPENAI_CONFIG.get('base_url'):
            client_kwargs['base_url'] = OPENAI_CONFIG['base_url']
        
        print(f"ğŸ”§ åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯")
        print(f"   Base URL: {client_kwargs.get('base_url', 'default')}")
        print(f"   Model: {OPENAI_CONFIG.get('model')}")
        
        try:
            self.client = OpenAI(**client_kwargs)
        except Exception as e:
            print(f"âŒ OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"   è¯·æ£€æŸ¥ openai å’Œ httpx ç‰ˆæœ¬æ˜¯å¦åŒ¹é…")
            raise
            
        self.model = OPENAI_CONFIG['model']
        self.temperature = OPENAI_CONFIG['temperature']
        self.max_tokens = OPENAI_CONFIG['max_tokens']
        self.max_retries = OPENAI_CONFIG['max_retries']
    
    def process_message(
        self,
        session_id: str,
        user_message: str,
        session_data: Dict
    ) -> Tuple[str, str]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯çš„ä¸»å…¥å£
        
        Args:
            session_id: ä¼šè¯ ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            session_data: ä¼šè¯æ•°æ®ï¼ˆåŒ…å« current_state, chat_history ç­‰ï¼‰
        
        Returns:
            (assistant_response, new_state): AI å›å¤å’Œæ–°çš„çŠ¶æ€
        """
        current_state = session_data.get('current_state', 'GUIDE_PENDING_REPORT')
        
        # ğŸ” å¦‚æœå½“å‰å¤„äºç« èŠ‚çŠ¶æ€ä¸”æœ‰æ–°æ¶ˆæ¯ï¼Œé‡æ–°è¿›è¡Œè·¯ç”±åˆ¤æ–­
        # æ³¨æ„ï¼šä¸åŒ…å« GUIDE_PENDING_PLANï¼Œå› ä¸ºéœ€è¦è®© guidance_agent è‡ªå·±åˆ¤æ–­åœºæ™¯äºŒ/åœºæ™¯ä¸‰
        chapter_states = ['INTRODUCTION', 'REVIEW', 'METHOD', 'RESULT', 'DISCUSSION', 'CONTROL_ROUTING']
        if current_state in chapter_states and user_message:
            print(f"ğŸ”„ æ£€æµ‹åˆ°æ–°é—®é¢˜ï¼Œä»çŠ¶æ€ {current_state} é‡æ–°è¿›è¡Œè·¯ç”±åˆ¤æ–­")
            return self._handle_control_routing(session_id, user_message, session_data)
        
        # 1. æ ¹æ®çŠ¶æ€è·å–å¯¹åº”çš„æ™ºèƒ½ä½“
        agent_name = self._get_agent_by_state(current_state)
        
        # 2. åŠ è½½æ™ºèƒ½ä½“çš„ Prompt
        system_prompt = prompt_manager.get_prompt(agent_name)
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå†å²å¯¹è¯ + è®ºæ–‡å†…å®¹ç­‰ï¼‰
        context = self._build_context(session_data)
        
        # 4. è°ƒç”¨ LLM
        assistant_response = self._call_llm(
            system_prompt=system_prompt,
            context=context,
            user_message=user_message,
            chat_history=session_data.get('session_data', {}).get('chat_history', [])
        )
        
        # ğŸ” è°ƒè¯•ï¼šæ‰“å° LLM å“åº”
        print(f"\n{'='*60}")
        print(f"ğŸ¤– æ™ºèƒ½ä½“ [{AGENT_DISPLAY_NAMES.get(agent_name, agent_name)}] å“åº”:")
        print(f"{'='*60}")
        print(f"å“åº”é•¿åº¦: {len(assistant_response)} å­—ç¬¦")
        if len(assistant_response) < 200:
            print(f"å“åº”å†…å®¹: {assistant_response}")
        else:
            print(f"å“åº”å‰200å­—: {assistant_response[:200]}...")
        print(f"{'='*60}\n")
        
        # ğŸ” æ£€æŸ¥ç©ºå“åº”
        if not assistant_response or len(assistant_response.strip()) == 0:
            print(f"âš ï¸  è­¦å‘Šï¼šæ™ºèƒ½ä½“è¿”å›äº†ç©ºå“åº”ï¼")
            print(f"   å½“å‰çŠ¶æ€: {current_state}")
            print(f"   æ™ºèƒ½ä½“: {agent_name}")
            print(f"   ç”¨æˆ·æ¶ˆæ¯: {user_message}")
            # è¿”å›å‹å¥½æç¤ºè€Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²
            assistant_response = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤ã€‚è¯·ç¨åå†è¯•æˆ–æ¢ä¸ªæ–¹å¼æé—®ã€‚"
        
        # ğŸ” æ£€æµ‹åœºæ™¯ä¸‰çš„ç‰¹æ®Šè·¯ç”±æ ‡è®°ï¼ˆguidance_agentä¸“ç”¨ï¼‰
        if agent_name == 'guidance' and current_state == 'GUIDE_PENDING_PLAN':
            import json
            import re
            try:
                # å°è¯•æå–JSONæ ‡è®°
                json_match = re.search(r'\{[^}]*"route"[^}]*\}', assistant_response)
                if json_match:
                    route_data = json.loads(json_match.group())
                    if route_data.get('route') == 'content_question':
                        print("ğŸ”„ æ£€æµ‹åˆ°åœºæ™¯ä¸‰è·¯ç”±æ ‡è®°ï¼Œè‡ªåŠ¨è½¬å‘åˆ° control_routing")
                        # ç›´æ¥è°ƒç”¨ control_routing å¤„ç†ç”¨æˆ·çš„åŸå§‹é—®é¢˜
                        # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥è¿”å›ï¼Œä¸å†è¿”å›åŒ…å«JSONçš„guidanceå›å¤
                        return self._handle_control_routing(session_id, user_message, session_data)
            except Exception as e:
                print(f"âš ï¸  è§£æè·¯ç”±æ ‡è®°å¤±è´¥ï¼ˆå¯èƒ½ä¸æ˜¯åœºæ™¯ä¸‰ï¼‰: {e}")
        
        # 5. åˆ¤æ–­æ˜¯å¦éœ€è¦çŠ¶æ€è½¬æ¢
        new_state = self._determine_next_state(
            current_state=current_state,
            user_message=user_message,
            assistant_response=assistant_response,
            session_data=session_data
        )
        
        return assistant_response, new_state
    
    def _handle_control_routing(
        self,
        session_id: str,
        user_message: str,
        session_data: Dict
    ) -> Tuple[str, str]:
        """
        å¤„ç†CONTROL_ROUTINGçŠ¶æ€ï¼šè®©control_agentå†³ç­–è·¯ç”±
        
        Args:
            session_id: ä¼šè¯ ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            session_data: ä¼šè¯æ•°æ®
        
        Returns:
            (assistant_response, new_state): AI å›å¤å’Œæ–°çš„çŠ¶æ€
        """
        # 1. è°ƒç”¨control_agentè¿›è¡Œè·¯ç”±å†³ç­–
        control_prompt = prompt_manager.get_prompt('control')
        context = self._build_context(session_data)
        
        routing_response = self._call_llm(
            system_prompt=control_prompt,
            context=context,
            user_message=f"ç”¨æˆ·é—®é¢˜ï¼š{user_message}\n\nè¯·åˆ†æè¿™ä¸ªé—®é¢˜åº”è¯¥è·¯ç”±ç»™å“ªä¸ªæ™ºèƒ½ä½“ã€‚",
            chat_history=[]
        )
        
        # ğŸ” æ‰“å°ä¸­æ§æ™ºèƒ½ä½“çš„å®Œæ•´å›å¤ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print("\n" + "="*60)
        print("ğŸ¯ ä¸­æ§æ™ºèƒ½ä½“ (Control Agent) è·¯ç”±å†³ç­–ï¼š")
        print("="*60)
        print(routing_response)
        print("="*60 + "\n")
        
        # 2. è§£æè·¯ç”±å†³ç­–ï¼ˆå°è¯•ä»JSONä¸­æå–agent_nameï¼‰
        import json
        import re
        
        target_agent = None
        try:
            # å°è¯•æå–JSONï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            # æ–¹å¼1: ç›´æ¥è§£ææ•´ä¸ªå“åº”
            try:
                routing_json = json.loads(routing_response.strip())
                target_agent = routing_json.get('agent_name')
            except:
                # æ–¹å¼2: æå–```jsonä»£ç å—
                json_block_match = re.search(r'```json\s*(\{[^}]+\})\s*```', routing_response, re.DOTALL)
                if json_block_match:
                    routing_json = json.loads(json_block_match.group(1))
                    target_agent = routing_json.get('agent_name')
                else:
                    # æ–¹å¼3: æå–ä»»æ„JSONå¯¹è±¡
                    json_match = re.search(r'\{[^}]+\}', routing_response)
                    if json_match:
                        routing_json = json.loads(json_match.group())
                        target_agent = routing_json.get('agent_name')
            
            if target_agent:
                print(f"âœ… è·¯ç”±å†³ç­–æˆåŠŸ: {target_agent}")
            else:
                print(f"âš ï¸  JSONè§£ææˆåŠŸä½†æœªæ‰¾åˆ°agent_nameå­—æ®µ")
                
        except Exception as e:
            print(f"âŒ è§£æè·¯ç”±å†³ç­–å¤±è´¥: {e}")
        
        # 3. å¦‚æœæ— æ³•è§£æï¼Œé»˜è®¤ä½¿ç”¨generalæ™ºèƒ½ä½“
        if not target_agent:
            print("âš ï¸  æ— æ³•è§£æè·¯ç”±å†³ç­–ï¼Œä½¿ç”¨generalä½œä¸ºé»˜è®¤")
            target_agent = 'general'
        
        # 3.5 æ ‡å‡†åŒ–agentåç§°ï¼šç§»é™¤_agentåç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # control_agentè¿”å›çš„å¯èƒ½æ˜¯ "review_agent"ï¼Œéœ€è¦è½¬æ¢ä¸º "review"
        if target_agent.endswith('_agent'):
            original_target = target_agent
            target_agent = target_agent.replace('_agent', '')
            print(f"ğŸ”„ æ ‡å‡†åŒ–agentåç§°: {original_target} -> {target_agent}")
        
        # 4. æ„å»ºcontextæ—¶ï¼ŒåŒ…å«agent_inquiry_statusä¿¡æ¯
        context_with_status = self._build_context_with_agent_status(session_data, target_agent)
        
        # 5. è°ƒç”¨ç›®æ ‡æ™ºèƒ½ä½“ï¼ˆä¸æ‰“å°å­æ™ºèƒ½ä½“å›å¤ï¼Œåªæ‰“å°controlï¼‰
        target_prompt = prompt_manager.get_prompt(target_agent)
        assistant_response = self._call_llm(
            system_prompt=target_prompt,
            context=context_with_status,
            user_message=user_message,
            chat_history=session_data.get('session_data', {}).get('chat_history', [])
        )
        
        # 6. æ›´æ–°agent_inquiry_statusï¼šæ ‡è®°è¯¥æ™ºèƒ½ä½“å·²è¢«è¯¢é—®
        # åœ¨æ›´æ–°å‰è®°å½•å½“å‰çŠ¶æ€ï¼Œç”¨äºåˆ¤æ–­æ¨¡å¼
        session_dict = session_data.get('session_data', {})
        agent_inquiry_status = session_dict.get('agent_inquiry_status', {})
        is_first_inquiry = not agent_inquiry_status.get(target_agent, False)
        
        self._update_agent_inquiry_status(session_id, target_agent)
        
        # ğŸ”§ è°ƒè¯•æ ‡ç­¾ï¼šåœ¨å›å¤æœ«å°¾æ·»åŠ æ™ºèƒ½ä½“å’Œæ¨¡å¼ä¿¡æ¯
        mode_text = "é¦–æ¬¡æ¨¡å¼" if is_first_inquiry else "å¸¸è§„æ¨¡å¼"
        # ç®€åŒ–çš„æ™ºèƒ½ä½“åç§°æ˜ å°„
        agent_short_names = {
            'introduction': 'å¼•è¨€',
            'review': 'ç»¼è¿°',
            'method': 'æ–¹æ³•',
            'result': 'ç»“æœ',
            'discussion': 'è®¨è®º',
            'general': 'é€šç”¨',
            'concept': 'æ¦‚å¿µ'
        }
        agent_short = agent_short_names.get(target_agent, target_agent)
        debug_tag = f"\n\n---\nã€{agent_short}ã€‘ã€{mode_text}ã€‘"
        assistant_response += debug_tag
        
        # 7. ç¡®å®šæ–°çŠ¶æ€ï¼ˆæ ¹æ®ç›®æ ‡agentæ˜ å°„åˆ°çŠ¶æ€ï¼‰
        agent_to_state = {
            'introduction': 'INTRODUCTION',
            'review': 'REVIEW',
            'method': 'METHOD',
            'result': 'RESULT',
            'discussion': 'DISCUSSION',
        }
        new_state = agent_to_state.get(target_agent, 'CONTROL_ROUTING')
        
        # 6. ç›´æ¥è¿”å›å­æ™ºèƒ½ä½“çš„å›ç­”ï¼ˆä¸å†å®¡æ ¸ï¼‰
        return assistant_response, new_state
    
    def _get_agent_by_state(self, state: str) -> str:
        """
        æ ¹æ® FSM çŠ¶æ€è·å–å¯¹åº”çš„æ™ºèƒ½ä½“åç§°
        
        Args:
            state: å½“å‰ FSM çŠ¶æ€
        
        Returns:
            agent_name: æ™ºèƒ½ä½“åç§°
        """
        agent_name = STATE_AGENT_MAPPING.get(state)
        
        if not agent_name:
            # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œä½¿ç”¨é€šç”¨æ™ºèƒ½ä½“
            print(f"âš ï¸  æœªçŸ¥çŠ¶æ€ {state}ï¼Œä½¿ç”¨é€šç”¨æ™ºèƒ½ä½“")
            return 'general_agent'
        
        return agent_name
    
    def _build_context(self, session_data: Dict) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        
        æ ¹æ®è®¾è®¡æ–‡æ¡£ï¼Œä¸Šä¸‹æ–‡åŒ…æ‹¬ï¼š
        1. ä¸»å†å²è®°å½•ï¼ˆmain historyï¼‰
        2. é€‰æ‹©æ€§ä¸Šä¸‹æ–‡åŒ…ï¼ˆcontext packagesï¼‰
        
        Args:
            session_data: ä¼šè¯æ•°æ®
        
        Returns:
            context: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        context_parts = []
        
        # 1. è®ºæ–‡åŸºæœ¬ä¿¡æ¯
        paper_path = session_data.get('paper_path')
        markdown_path = session_data.get('markdown_path')
        
        print(f"ğŸ“‹ æ„å»ºä¸Šä¸‹æ–‡ - paper_path: {paper_path}")
        print(f"ğŸ“‹ æ„å»ºä¸Šä¸‹æ–‡ - markdown_path: {markdown_path}")
        
        if paper_path:
            context_parts.append(f"ğŸ“„ è®ºæ–‡æ–‡ä»¶: {paper_path}")
        
        # 2. Markdown å†…å®¹ï¼ˆå¦‚æœå·²è½¬æ¢ï¼‰
        if markdown_path and os.path.exists(markdown_path):
            try:
                with open(markdown_path, 'r', encoding='utf-8') as f:
                    markdown_content = f.read()
                    content_length = len(markdown_content)
                    print(f"ğŸ“‹ è¯»å–åˆ° Markdown å†…å®¹ï¼Œé•¿åº¦: {content_length} å­—ç¬¦")
                    # æˆªå–å‰ 50000 å­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆå¢åŠ åˆ° 5 å€ï¼Œçº¦ 50KBï¼‰
                    if len(markdown_content) > 50000:
                        markdown_content = markdown_content[:50000] + "\n\n... (å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)"
                    context_parts.append(f"ğŸ“ è®ºæ–‡å†…å®¹:\n{markdown_content}")
            except Exception as e:
                print(f"âš ï¸  è¯»å– Markdown å¤±è´¥: {e}")
        else:
            if markdown_path:
                print(f"âš ï¸  Markdown æ–‡ä»¶ä¸å­˜åœ¨: {markdown_path}")
        
        # 3. ä¸Šä¸‹æ–‡åŒ…ï¼ˆä» session_data ä¸­è¯»å–ï¼‰
        session_dict = session_data.get('session_data', {})
        context_packages = session_dict.get('context_packages', {})
        
        if context_packages:
            context_parts.append("ğŸ“¦ ä¸Šä¸‹æ–‡åŒ…:")
            for key, value in context_packages.items():
                context_parts.append(f"  - {key}: {value}")
        
        # 4. é˜…è¯»è®¡åˆ’ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        reading_plan = session_dict.get('reading_plan')
        if reading_plan:
            context_parts.append(f"ğŸ“‹ é˜…è¯»è®¡åˆ’:\n{reading_plan}")
        
        return "\n\n".join(context_parts) if context_parts else "æš‚æ— ä¸Šä¸‹æ–‡ä¿¡æ¯"
    
    def _build_context_with_agent_status(self, session_data: Dict, target_agent: str) -> str:
        """
        æ„å»ºåŒ…å«æ™ºèƒ½ä½“è¯¢é—®çŠ¶æ€çš„ä¸Šä¸‹æ–‡
        
        Args:
            session_data: ä¼šè¯æ•°æ®
            target_agent: ç›®æ ‡æ™ºèƒ½ä½“åç§°
        
        Returns:
            context: åŒ…å«çŠ¶æ€ä¿¡æ¯çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        # è·å–åŸºç¡€ä¸Šä¸‹æ–‡
        base_context = self._build_context(session_data)
        
        # è·å–agent_inquiry_status
        session_dict = session_data.get('session_data', {})
        agent_inquiry_status = session_dict.get('agent_inquiry_status', {})
        
        # åˆ¤æ–­è¯¥æ™ºèƒ½ä½“æ˜¯å¦å·²è¢«è¯¢é—®
        is_first_inquiry = not agent_inquiry_status.get(target_agent, False)
        
        # æ·»åŠ æ¨¡å—äº¤äº’çŠ¶æ€ä¿¡æ¯ï¼ˆæ ¼å¼æ›´æ¸…æ™°ï¼‰
        status_text = f"\n\n===== é‡è¦ï¼šæ¨¡å—äº¤äº’çŠ¶æ€ =====\n"
        status_text += f"å½“å‰æ¨¡å—: {AGENT_DISPLAY_NAMES.get(target_agent, target_agent)}\n"
        status_text += f"äº¤äº’çŠ¶æ€: {'æœªè¯¢é—®ï¼ˆé¦–æ¬¡è¯¢é—®ï¼‰' if is_first_inquiry else 'å·²è¯¢é—®ï¼ˆéé¦–æ¬¡è¯¢é—®ï¼‰'}\n"
        if is_first_inquiry:
            status_text += "âš ï¸ è¿™æ˜¯ç”¨æˆ·é¦–æ¬¡è¯¢é—®æ­¤æ¨¡å—ï¼Œè¯·ä½¿ç”¨ã€Œé¦–æ¬¡å¼•å¯¼æ¨¡å¼ã€è¾“å‡ºï¼\n"
        else:
            status_text += "âœ… ç”¨æˆ·å·²è¯¢é—®è¿‡æ­¤æ¨¡å—ï¼Œè¯·ä½¿ç”¨ã€Œå¸¸è§„æ¨¡å¼ã€è¾“å‡ºï¼\n"
        status_text += "============================="
        
        print(f"\nğŸ” æ™ºèƒ½ä½“çŠ¶æ€æ£€æµ‹:")
        print(f"   ç›®æ ‡æ™ºèƒ½ä½“: {target_agent}")
        print(f"   æ˜¯å¦é¦–æ¬¡è¯¢é—®: {is_first_inquiry}")
        print(f"   å½“å‰çŠ¶æ€è®°å½•: {agent_inquiry_status}")
        
        return f"{base_context}{status_text}"
    
    def _update_agent_inquiry_status(self, session_id: str, agent_name: str):
        """
        æ›´æ–°æ™ºèƒ½ä½“è¯¢é—®çŠ¶æ€
        
        Args:
            session_id: ä¼šè¯ID
            agent_name: æ™ºèƒ½ä½“åç§°
        """
        from db import db
        
        # è·å–å½“å‰session_data
        session_data = db.get_session(session_id)
        if not session_data:
            return
        
        session_dict = session_data.get('session_data', {})
        agent_inquiry_status = session_dict.get('agent_inquiry_status', {})
        
        # æ›´æ–°çŠ¶æ€
        agent_inquiry_status[agent_name] = True
        session_dict['agent_inquiry_status'] = agent_inquiry_status
        
        # ä¿å­˜å›æ•°æ®åº“
        db.update_session(session_id, session_data=session_dict)
        
        print(f"âœ… å·²æ›´æ–° {agent_name} çš„è¯¢é—®çŠ¶æ€ä¸ºï¼šå·²è¯¢é—®")
    
    def _call_llm(
        self,
        system_prompt: str,
        context: str,
        user_message: str,
        chat_history: List[Dict]
    ) -> str:
        """
        è°ƒç”¨ OpenAI APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿ Promptï¼ˆæ™ºèƒ½ä½“çš„è§’è‰²å®šä¹‰ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            chat_history: å†å²å¯¹è¯
        
        Returns:
            assistant_response: AI å›å¤
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Gemini æ¨¡å‹
        is_gemini = 'gemini' in self.model.lower()
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        if is_gemini:
            # Gemini ä¸æ”¯æŒ system è§’è‰²ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            # å°† system prompt å’Œ context åˆå¹¶åˆ°ç¬¬ä¸€æ¡ user æ¶ˆæ¯ä¸­
            system_content = system_prompt
            if context:
                system_content += f"\n\nä»¥ä¸‹æ˜¯å½“å‰ä¼šè¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n\n{context}"
            
            messages = []
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆåªä¿ç•™æœ€è¿‘ 10 è½®ï¼‰
            recent_history = chat_history[-20:] if len(chat_history) > 20 else chat_history
            for msg in recent_history:
                role = msg.get("role", "user")
                # Gemini åªæ”¯æŒ user å’Œ model (assistant)
                if role == "assistant":
                    role = "model"
                messages.append({
                    "role": role,
                    "content": msg.get("content", "")
                })
            
            # å°† system prompt å’Œç”¨æˆ·æ¶ˆæ¯åˆå¹¶
            combined_user_message = f"{system_content}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"
            messages.append({
                "role": "user",
                "content": combined_user_message
            })
        else:
            # OpenAI æ ‡å‡†æ ¼å¼
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                }
            ]
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆä½œä¸ºç³»ç»Ÿæ¶ˆæ¯ï¼‰
            if context:
                messages.append({
                    "role": "system",
                    "content": f"ä»¥ä¸‹æ˜¯å½“å‰ä¼šè¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n\n{context}"
                })
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆåªä¿ç•™æœ€è¿‘ 10 è½®ï¼Œé¿å…è¶…é•¿ï¼‰
            recent_history = chat_history[-20:] if len(chat_history) > 20 else chat_history
            for msg in recent_history:
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", "")
                })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({
                "role": "user",
                "content": user_message
            })
        
        # é‡è¯•é€»è¾‘
        for attempt in range(self.max_retries):
            try:
                # è°ƒç”¨ OpenAI API
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )
                
                # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
                if not response.choices or len(response.choices) == 0:
                    raise ValueError("API è¿”å›çš„ choices åˆ—è¡¨ä¸ºç©º")
                
                assistant_response = response.choices[0].message.content
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
                if assistant_response is None:
                    raise ValueError("API è¿”å›çš„å†…å®¹ä¸º None")
                
                return assistant_response
            
            except Exception as e:
                print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                print(f"   æ¨¡å‹: {self.model}")
                print(f"   æ¶ˆæ¯æ•°é‡: {len(messages)}")
                
                if attempt < self.max_retries - 1:
                    # æŒ‡æ•°é€€é¿
                    wait_time = 2 ** attempt
                    print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
                    return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜æ— æ³•å›å¤ã€‚è¯·ç¨åå†è¯•ã€‚\n\né”™è¯¯è¯¦æƒ…ï¼š{str(e)}"
    
    def _call_llm_stream(
        self,
        system_prompt: str,
        context: str,
        user_message: str,
        chat_history: List[Dict]
    ):
        """
        è°ƒç”¨ OpenAI APIï¼ˆæµå¼è¾“å‡ºï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿ Promptï¼ˆæ™ºèƒ½ä½“çš„è§’è‰²å®šä¹‰ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            chat_history: å†å²å¯¹è¯
        
        Yields:
            str: æµå¼è¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Gemini æ¨¡å‹
        is_gemini = 'gemini' in self.model.lower()
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸éæµå¼æ–¹æ³•ç›¸åŒï¼‰
        if is_gemini:
            system_content = system_prompt
            if context:
                system_content += f"\n\nä»¥ä¸‹æ˜¯å½“å‰ä¼šè¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n\n{context}"
            
            messages = []
            recent_history = chat_history[-20:] if len(chat_history) > 20 else chat_history
            for msg in recent_history:
                role = msg.get("role", "user")
                if role == "assistant":
                    role = "model"
                messages.append({
                    "role": role,
                    "content": msg.get("content", "")
                })
            
            combined_user_message = f"{system_content}\n\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"
            messages.append({
                "role": "user",
                "content": combined_user_message
            })
        else:
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                }
            ]
            
            if context:
                messages.append({
                    "role": "system",
                    "content": f"ä»¥ä¸‹æ˜¯å½“å‰ä¼šè¯çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n\n{context}"
                })
            
            recent_history = chat_history[-20:] if len(chat_history) > 20 else chat_history
            for msg in recent_history:
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", "")
                })
            
            messages.append({
                "role": "user",
                "content": user_message
            })
        
        try:
            # è°ƒç”¨ OpenAI API (æµå¼)
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True  # å¯ç”¨æµå¼è¾“å‡º
            )
            
            # é€å—è¿”å›å†…å®¹
            for chunk in stream:
                # æ£€æŸ¥ chunk æ˜¯å¦æœ‰ choices
                if not chunk.choices or len(chunk.choices) == 0:
                    continue
                
                # æ£€æŸ¥ delta æ˜¯å¦æœ‰ content
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content is not None:
                    yield delta.content
            
        except Exception as e:
            print(f"âŒ æµå¼ API è°ƒç”¨å¤±è´¥: {e}")
            print(f"   æ¨¡å‹: {self.model}")
            yield f"\n\næŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜æ— æ³•å›å¤ã€‚è¯·ç¨åå†è¯•ã€‚\n\né”™è¯¯è¯¦æƒ…ï¼š{str(e)}"
    
    def process_message_stream(
        self,
        session_id: str,
        user_message: str,
        session_data: Dict
    ):
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ˆæµå¼è¾“å‡ºç‰ˆæœ¬ï¼‰
        
        Args:
            session_id: ä¼šè¯ ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            session_data: ä¼šè¯æ•°æ®
        
        Yields:
            dict: åŒ…å« content å’Œ state çš„å­—å…¸
        """
        current_state = session_data.get('current_state', 'GUIDE_PENDING_REPORT')
        
        # çŠ¶æ€è·¯ç”±é€»è¾‘ï¼šç« èŠ‚çŠ¶æ€ç›´æ¥è·¯ç”±
        chapter_states = ['INTRODUCTION', 'REVIEW', 'METHOD', 'RESULT', 'DISCUSSION', 'CONTROL_ROUTING']
        if current_state in chapter_states and user_message:
            # æµå¼ç‰ˆæœ¬ï¼šé€å­—è¿”å›
            for chunk_data in self._handle_control_routing_stream(session_id, user_message, session_data):
                yield chunk_data
            return
        
        # è·å–æ™ºèƒ½ä½“å’Œ Prompt
        agent_name = self._get_agent_by_state(current_state)
        system_prompt = prompt_manager.get_prompt(agent_name)
        context = self._build_context(session_data)
        
        # ç‰¹æ®Šå¤„ç† guidance_agent çš„åœºæ™¯ä¸‰ï¼šå…ˆå®Œæ•´æ”¶é›†å“åº”ï¼Œæ£€æµ‹è·¯ç”±æ ‡è®°
        if agent_name == 'guidance' and current_state == 'GUIDE_PENDING_PLAN':
            # å…ˆæ”¶é›†å®Œæ•´å“åº”ï¼ˆä¸æµå¼è¾“å‡ºï¼‰
            full_response = ""
            for chunk in self._call_llm_stream(
                system_prompt=system_prompt,
                context=context,
                user_message=user_message,
                chat_history=session_data.get('session_data', {}).get('chat_history', [])
            ):
                full_response += chunk
            
            # æ£€æµ‹åœºæ™¯ä¸‰çš„è·¯ç”±æ ‡è®°
            import json
            import re
            try:
                json_match = re.search(r'\{[^}]*"route"[^}]*\}', full_response)
                if json_match:
                    route_data = json.loads(json_match.group())
                    if route_data.get('route') == 'content_question':
                        print("ğŸ”„ æ£€æµ‹åˆ°åœºæ™¯ä¸‰è·¯ç”±æ ‡è®°ï¼Œè‡ªåŠ¨è½¬å‘åˆ° control_routing")
                        # ç›´æ¥æµå¼è¾“å‡ºçœŸæ­£çš„ç­”æ¡ˆï¼Œä¸è¾“å‡º guidance çš„å“åº”ï¼ˆåŒ…å«JSONï¼‰
                        for chunk_data in self._handle_control_routing_stream(session_id, user_message, session_data):
                            yield chunk_data
                        return
            except Exception as e:
                print(f"âš ï¸  è§£æè·¯ç”±æ ‡è®°å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰è·¯ç”±æ ‡è®°ï¼Œæ­£å¸¸æµå¼è¾“å‡º guidance çš„å“åº”
            for i, char in enumerate(full_response):
                yield {'content': char, 'done': False}
            yield {'event': 'done', 'state': self._determine_next_state(
                current_state=current_state,
                user_message=user_message,
                assistant_response=full_response,
                session_data=session_data
            ), 'done': True}
            return
        
        # å…¶ä»–æ™ºèƒ½ä½“ï¼šæ­£å¸¸æµå¼è°ƒç”¨ LLM
        full_response = ""
        for chunk in self._call_llm_stream(
            system_prompt=system_prompt,
            context=context,
            user_message=user_message,
            chat_history=session_data.get('session_data', {}).get('chat_history', [])
        ):
            full_response += chunk
            yield {'content': chunk, 'done': False}
        
        # ğŸ”§ è°ƒè¯•æ ‡ç­¾ï¼šä»…ä¸ºé guidance æ™ºèƒ½ä½“æ·»åŠ ï¼ˆå¯¼è¯»æ™ºèƒ½ä½“ä¸éœ€è¦ï¼‰
        if agent_name != 'guidance':
            # è·å–æ™ºèƒ½ä½“çŠ¶æ€ï¼Œåˆ¤æ–­æ¨¡å¼
            session_dict = session_data.get('session_data', {})
            agent_inquiry_status = session_dict.get('agent_inquiry_status', {})
            is_first_inquiry = not agent_inquiry_status.get(agent_name, False)
            
            mode_text = "é¦–æ¬¡æ¨¡å¼" if is_first_inquiry else "å¸¸è§„æ¨¡å¼"
            # ç®€åŒ–çš„æ™ºèƒ½ä½“åç§°æ˜ å°„
            agent_short_names = {
                'introduction': 'å¼•è¨€',
                'review': 'ç»¼è¿°',
                'method': 'æ–¹æ³•',
                'result': 'ç»“æœ',
                'discussion': 'è®¨è®º',
                'general': 'é€šç”¨',
                'concept': 'æ¦‚å¿µ'
            }
            agent_short = agent_short_names.get(agent_name, agent_name)
            debug_tag = f"\n\n---\nã€{agent_short}ã€‘ã€{mode_text}ã€‘"
            
            # é€å­—è¾“å‡ºè°ƒè¯•æ ‡ç­¾
            for char in debug_tag:
                yield {'content': char, 'done': False}
            
            full_response += debug_tag
        
        # åˆ¤æ–­æ–°çŠ¶æ€
        new_state = self._determine_next_state(
            current_state=current_state,
            user_message=user_message,
            assistant_response=full_response,
            session_data=session_data
        )
        
        # å‘é€å®Œæˆä¿¡å·
        yield {'done': True, 'state': new_state, 'full_response': full_response}
    
    def _handle_control_routing_stream(
        self,
        session_id: str,
        user_message: str,
        session_data: Dict
    ):
        """
        å¤„ç†CONTROL_ROUTINGçŠ¶æ€ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
        
        Args:
            session_id: ä¼šè¯ ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            session_data: ä¼šè¯æ•°æ®
        
        Yields:
            dict: åŒ…å« content å’Œ state çš„å­—å…¸
        """
        # 1. è°ƒç”¨control_agentè¿›è¡Œè·¯ç”±å†³ç­–ï¼ˆéæµå¼ï¼Œå¿«é€Ÿåˆ¤æ–­ï¼‰
        control_prompt = prompt_manager.get_prompt('control')
        context = self._build_context(session_data)
        
        routing_response = self._call_llm(
            system_prompt=control_prompt,
            context=context,
            user_message=f"ç”¨æˆ·é—®é¢˜ï¼š{user_message}\n\nè¯·åˆ†æè¿™ä¸ªé—®é¢˜åº”è¯¥è·¯ç”±ç»™å“ªä¸ªæ™ºèƒ½ä½“ã€‚",
            chat_history=[]
        )
        
        print("\n" + "="*60)
        print("ğŸ¯ ä¸­æ§æ™ºèƒ½ä½“ (Control Agent) è·¯ç”±å†³ç­–ï¼š")
        print("="*60)
        print(routing_response)
        print("="*60 + "\n")
        
        # 2. è§£æè·¯ç”±å†³ç­–
        import json
        import re
        
        target_agent = None
        try:
            # å°è¯•æå–JSON
            try:
                routing_json = json.loads(routing_response.strip())
                target_agent = routing_json.get('agent_name')
            except:
                json_block_match = re.search(r'```json\s*(\{[^}]+\})\s*```', routing_response, re.DOTALL)
                if json_block_match:
                    routing_json = json.loads(json_block_match.group(1))
                    target_agent = routing_json.get('agent_name')
                else:
                    json_match = re.search(r'\{[^}]+\}', routing_response)
                    if json_match:
                        routing_json = json.loads(json_match.group())
                        target_agent = routing_json.get('agent_name')
            
            if target_agent:
                print(f"âœ… è·¯ç”±å†³ç­–æˆåŠŸ: {target_agent}")
        except Exception as e:
            print(f"âŒ è§£æè·¯ç”±å†³ç­–å¤±è´¥: {e}")
        
        if not target_agent:
            print("âš ï¸  æ— æ³•è§£æè·¯ç”±å†³ç­–ï¼Œä½¿ç”¨generalä½œä¸ºé»˜è®¤")
            target_agent = 'general'
        
        # æ ‡å‡†åŒ–agentåç§°
        if target_agent.endswith('_agent'):
            target_agent = target_agent.replace('_agent', '')
        
        # 3. æ„å»ºcontextå¹¶è°ƒç”¨ç›®æ ‡æ™ºèƒ½ä½“ï¼ˆæµå¼è¾“å‡ºï¼‰
        context_with_status = self._build_context_with_agent_status(session_data, target_agent)
        target_prompt = prompt_manager.get_prompt(target_agent)
        
        # åœ¨è°ƒç”¨å‰è®°å½•å½“å‰çŠ¶æ€ï¼Œç”¨äºåˆ¤æ–­æ¨¡å¼
        session_dict = session_data.get('session_data', {})
        agent_inquiry_status = session_dict.get('agent_inquiry_status', {})
        is_first_inquiry = not agent_inquiry_status.get(target_agent, False)
        
        full_response = ""
        for chunk in self._call_llm_stream(
            system_prompt=target_prompt,
            context=context_with_status,
            user_message=user_message,
            chat_history=session_data.get('session_data', {}).get('chat_history', [])
        ):
            full_response += chunk
            yield {'content': chunk, 'done': False}
        
        # ğŸ”§ è°ƒè¯•æ ‡ç­¾ï¼šåœ¨æµå¼è¾“å‡ºæœ«å°¾æ·»åŠ æ™ºèƒ½ä½“å’Œæ¨¡å¼ä¿¡æ¯
        mode_text = "é¦–æ¬¡æ¨¡å¼" if is_first_inquiry else "å¸¸è§„æ¨¡å¼"
        # ç®€åŒ–çš„æ™ºèƒ½ä½“åç§°æ˜ å°„
        agent_short_names = {
            'introduction': 'å¼•è¨€',
            'review': 'ç»¼è¿°',
            'method': 'æ–¹æ³•',
            'result': 'ç»“æœ',
            'discussion': 'è®¨è®º',
            'general': 'é€šç”¨',
            'concept': 'æ¦‚å¿µ'
        }
        agent_short = agent_short_names.get(target_agent, target_agent)
        debug_tag = f"\n\n---\nã€{agent_short}ã€‘ã€{mode_text}ã€‘"
        
        # é€å­—è¾“å‡ºè°ƒè¯•æ ‡ç­¾
        for char in debug_tag:
            yield {'content': char, 'done': False}
        
        full_response += debug_tag
        
        # 4. æ›´æ–°agent_inquiry_status
        self._update_agent_inquiry_status(session_id, target_agent)
        
        # 5. ç¡®å®šæ–°çŠ¶æ€
        agent_to_state = {
            'introduction': 'INTRODUCTION',
            'review': 'REVIEW',
            'method': 'METHOD',
            'result': 'RESULT',
            'discussion': 'DISCUSSION',
        }
        new_state = agent_to_state.get(target_agent, 'CONTROL_ROUTING')
        
        # 6. å‘é€å®Œæˆä¿¡å·
        yield {'done': True, 'state': new_state, 'full_response': full_response}
    
    def _determine_next_state(
        self,
        current_state: str,
        user_message: str,
        assistant_response: str,
        session_data: Dict
    ) -> str:
        """
        åˆ¤æ–­ä¸‹ä¸€ä¸ªçŠ¶æ€
        
        æ ¹æ®è®¾è®¡æ–‡æ¡£çš„ FSM æµç¨‹ï¼š
        GUIDE_PENDING_REPORT â†’ GUIDE_PENDING_PLAN â†’ CONTROL_ROUTING â†’ å…·ä½“ç« èŠ‚æ™ºèƒ½ä½“
        
        Args:
            current_state: å½“å‰çŠ¶æ€
            user_message: ç”¨æˆ·æ¶ˆæ¯
            assistant_response: AI å›å¤
            session_data: ä¼šè¯æ•°æ®
        
        Returns:
            new_state: æ–°çŠ¶æ€
        """
        # ç®€åŒ–çš„çŠ¶æ€è½¬æ¢é€»è¾‘ï¼ˆå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚æ‰©å±•ï¼‰
        
        print(f"ğŸ”„ çŠ¶æ€è½¬æ¢åˆ¤æ–­ - å½“å‰çŠ¶æ€: {current_state}")
        print(f"ğŸ”„ ç”¨æˆ·æ¶ˆæ¯: '{user_message}'")
        print(f"ğŸ”„ åŠ©æ‰‹å›å¤é•¿åº¦: {len(assistant_response)} å­—ç¬¦")
        
        # 1. GUIDE_PENDING_REPORT: ç­‰å¾…ç”¨æˆ·ä¸Šä¼ è®ºæ–‡æˆ–æé—®
        if current_state == 'GUIDE_PENDING_REPORT':
            # å¦‚æœå·²ç»æœ‰è®ºæ–‡ï¼Œå¹¶ä¸”æ˜¯ç©ºæ¶ˆæ¯è§¦å‘ï¼ˆé¦–æ¬¡è‡ªåŠ¨è§¦å‘ï¼‰
            if session_data.get('paper_path') and user_message == '':
                print("ğŸ”„ æ£€æµ‹åˆ°é¦–æ¬¡è‡ªåŠ¨è§¦å‘ï¼ˆç©ºæ¶ˆæ¯ï¼‰ï¼Œè½¬åˆ° GUIDE_PENDING_PLAN ç­‰å¾…ç”¨æˆ·å›åº”")
                # ç”Ÿæˆåˆå§‹æŠ¥å‘Šåï¼Œè½¬åˆ° GUIDE_PENDING_PLANï¼Œç­‰å¾…ç”¨æˆ·å›å¤å…´è¶£ç‚¹
                return 'GUIDE_PENDING_PLAN'
            
            # å¦‚æœç”¨æˆ·å·²ç»å›å¤äº†ï¼ˆéç©ºæ¶ˆæ¯ï¼‰ï¼Œéœ€è¦åˆ¤æ–­å›å¤ç±»å‹
            if session_data.get('paper_path') and user_message != '':
                # æ£€æµ‹æ˜¯å¦æ˜¯å†…å®¹æ€§é—®é¢˜ï¼ˆåœºæ™¯ä¸‰ï¼‰ - è¿™éƒ¨åˆ†é€»è¾‘å·²ç»åœ¨ process_message ä¸­å¤„ç†
                # å¦‚æœæ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜æ˜¯åœºæ™¯ä¸€æˆ–åœºæ™¯äºŒï¼Œåº”è¯¥ç”Ÿæˆé˜…è¯»è·¯å¾„åè½¬åˆ° CONTROL_ROUTING
                print("ğŸ”„ ç”¨æˆ·å›å¤äº†ç›®æ ‡/èƒŒæ™¯ä¿¡æ¯ï¼Œä¿æŒåœ¨ GUIDE_PENDING_PLAN")
                return 'GUIDE_PENDING_PLAN'
            
            return current_state
        
        # 2. GUIDE_PENDING_PLAN: å¼•å¯¼æ™ºèƒ½ä½“ç”Ÿæˆé˜…è¯»è®¡åˆ’
        if current_state == 'GUIDE_PENDING_PLAN':
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿›å…¥è¿™ä¸ªçŠ¶æ€ï¼ˆç©ºæ¶ˆæ¯ï¼‰ï¼Œç­‰å¾…ç”¨æˆ·å›å¤
            if user_message == '':
                print("ğŸ”„ ç¬¬ä¸€æ¬¡è¿›å…¥ GUIDE_PENDING_PLANï¼ˆå¯¼è¯»æŠ¥å‘Šå·²ç”Ÿæˆï¼‰ï¼Œç­‰å¾…ç”¨æˆ·å›å¤")
                return current_state
            
            # å¦‚æœç”¨æˆ·å·²å›å¤ï¼Œæ£€æµ‹æ˜¯å¦å·²ç”Ÿæˆé˜…è¯»è®¡åˆ’
            if user_message != '':
                # ç”Ÿæˆäº†ä¸ªæ€§åŒ–é˜…è¯»è·¯å¾„æˆ–æ²™æ¼å¼é˜…è¯»æ³•åï¼Œè½¬åˆ° CONTROL_ROUTING
                print("ğŸ”„ ç”¨æˆ·å·²å›å¤ç›®æ ‡ä¿¡æ¯ï¼Œå¼•å¯¼æ™ºèƒ½ä½“åº”ç”Ÿæˆè·¯å¾„ï¼Œè½¬åˆ° CONTROL_ROUTING")
                return 'CONTROL_ROUTING'
            
            return current_state
        
        # 3. CONTROL_ROUTING: ä¸­æ§æ™ºèƒ½ä½“è·¯ç”±åˆ°å…·ä½“ç« èŠ‚
        # æ³¨æ„ï¼šCONTROL_ROUTINGçŠ¶æ€çš„å¤„ç†å·²ç»åœ¨process_messageä¸­ç‰¹æ®Šå¤„ç†
        # è¿™é‡Œä¸åº”è¯¥è¢«æ‰§è¡Œåˆ°ï¼Œå› ä¸ºä¼šåœ¨_handle_control_routingä¸­å®Œæˆè·¯ç”±
        if current_state == 'CONTROL_ROUTING':
            # ä¿æŒåœ¨ CONTROL_ROUTINGï¼Œç­‰å¾…ä¸‹æ¬¡è·¯ç”±
            return current_state
        
        # 4. å…·ä½“ç« èŠ‚æ™ºèƒ½ä½“ï¼šä¿æŒå½“å‰çŠ¶æ€ï¼Œç­‰å¾…ç”¨æˆ·åˆ‡æ¢
        # ï¼ˆå¯ä»¥æ‰©å±•ä¸ºè‡ªåŠ¨æ£€æµ‹ä½•æ—¶è¿”å› CONTROL_ROUTINGï¼‰
        return current_state
    
    def force_state_transition(self, session_id: str, new_state: str) -> bool:
        """
        å¼ºåˆ¶çŠ¶æ€è½¬æ¢ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰
        
        Args:
            session_id: ä¼šè¯ ID
            new_state: æ–°çŠ¶æ€
        
        Returns:
            success: æ˜¯å¦æˆåŠŸ
        """
        try:
            db.update_session(session_id, current_state=new_state)
            print(f"âœ… ä¼šè¯ {session_id} çŠ¶æ€å·²æ›´æ–°ä¸º {new_state}")
            return True
        except Exception as e:
            print(f"âŒ çŠ¶æ€è½¬æ¢å¤±è´¥: {e}")
            return False


# åˆ›å»ºå…¨å±€å®ä¾‹
try:
    orchestrator = AgentOrchestrator()
    print("âœ… æ™ºèƒ½ä½“ç¼–æ’å™¨åˆå§‹åŒ–æˆåŠŸ")
except ValueError as e:
    print(f"âš ï¸  æ™ºèƒ½ä½“ç¼–æ’å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    print("    è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    orchestrator = None
